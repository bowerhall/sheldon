# Sheldon Configuration
#
# Local dev: cp .env.example .env && fill in values
# Deployment: Import to Doppler (see docs/deployment.md)

# =============================================================================
# REQUIRED
# =============================================================================

TELEGRAM_TOKEN=your-telegram-bot-token
OWNER_CHAT_ID=your-telegram-chat-id
KIMI_API_KEY=your-kimi-api-key
TZ=UTC

# =============================================================================
# OPTIONAL - Discord Bot
# Can run alongside or instead of Telegram
# =============================================================================

# DISCORD_TOKEN=your-discord-bot-token
# DISCORD_GUILD_ID=your-server-id
# DISCORD_OWNER_ID=your-user-id          # DMs with this user get full access
# DISCORD_TRUSTED_CHANNEL=channel-id     # Alternative: this channel gets full access

# =============================================================================
# OPTIONAL - LLM Provider
# Default is Kimi. Uncomment to use Claude or OpenAI instead.
# Can be switched at runtime via switch_model tool (requires API key set here).
# =============================================================================

# LLM_PROVIDER=claude
# LLM_MODEL=claude-sonnet-4-20250514
# ANTHROPIC_API_KEY=your-anthropic-api-key

# LLM_PROVIDER=openai
# LLM_MODEL=gpt-4o
# OPENAI_API_KEY=your-openai-api-key

# =============================================================================
# OPTIONAL - Coder LLM
# Uses KIMI_API_KEY by default. Set NVIDIA_API_KEY for free tier access.
# Can be changed at runtime via switch_model tool.
# =============================================================================

# NVIDIA_API_KEY=your-nvidia-api-key
# CODER_PROVIDER=kimi
# CODER_MODEL=kimi-k2.5:cloud

# =============================================================================
# OPTIONAL - Git Integration
# For coder to push code to GitHub
# =============================================================================

# GIT_TOKEN=your-github-pat
# GIT_USER_NAME=Sheldon
# GIT_USER_EMAIL=sheldon@example.com
# GIT_ORG_URL=https://github.com/your-org

# =============================================================================
# OPTIONAL - Budget & Usage Tracking
# Tracks API costs and enforces daily token limits.
# Enabled by default. Use usage_summary tool to check costs.
# =============================================================================

# BUDGET_DAILY_LIMIT=500000
# BUDGET_WARN_AT=0.8

# Conversation buffer size (number of recent messages to keep in context)
# Higher = more context continuity, but uses more tokens
# CONVERSATION_BUFFER_SIZE=24

# Agent max iterations (tool call rounds per message, default 20)
# Increase for complex multi-step skills
# AGENT_MAX_ITERATIONS=20

# =============================================================================
# OPTIONAL - Alert Chat ID
# Where to send budget warnings and error alerts
# Check-ins are now handled via the cron system (just ask Sheldon to check in)
# =============================================================================

# HEARTBEAT_CHAT_ID=your-telegram-chat-id

# =============================================================================
# OPTIONAL - Ollama (Local Models)
# Used for embeddings, extraction, and any local models.
# Model management tools (list_models, pull_model) connect here.
# =============================================================================

# OLLAMA_HOST=http://localhost:11434

# Preferred local models for auto-fallback (comma-separated, in order)
# Only used when all cloud providers are exhausted and ollama is available
# OLLAMA_FALLBACK_MODELS=llama3.2,qwen2.5:7b,mistral

# =============================================================================
# OPTIONAL - Extractor
# In deployment: uses local Ollama (qwen2:0.5b) - zero API cost
# For local dev: defaults to Kimi if not set
# =============================================================================

# EXTRACTOR_PROVIDER=ollama
# EXTRACTOR_BASE_URL=http://localhost:11434
# EXTRACTOR_MODEL=qwen2:0.5b

# =============================================================================
# OPTIONAL - Browser Sandbox
# Enabled by default. Uses isolated container for JS rendering.
# Falls back to HTTP if container unavailable.
# =============================================================================

BROWSER_SANDBOX_ENABLED=true
# BROWSER_SANDBOX_IMAGE=sheldon-browser-sandbox:latest

# =============================================================================
# OPTIONAL - Headscale (Self-hosted Tailscale)
# Enables Sheldon to connect to homelabs and remote GPU machines.
# Config is auto-initialized on first deploy.
#
# To add machines to your network:
#   1. Generate auth key: docker compose exec headscale headscale preauthkeys create --user 1 --expiration 24h
#   2. On remote machine:
#      HEADSCALE_URL=https://hs.yourdomain.com AUTHKEY=<key> \
#      curl -fsSL https://raw.githubusercontent.com/bowerhall/sheldon/main/core/scripts/invite.sh | sudo bash
#
# Or just ask Sheldon: "generate a homelab invite link"
# =============================================================================

# DOMAIN=example.com
# HEADSCALE_URL=https://hs.example.com

# =============================================================================
# OPTIONAL - Traefik Dashboard
# Access at traefik.yourdomain.com (protected by basic auth)
# Generate: htpasswd -nb admin yourpassword
# IMPORTANT: Double all $ signs (e.g. $apr1$ becomes $$apr1$$)
# =============================================================================

# TRAEFIK_DASHBOARD_AUTH=admin:$$apr1$$xxxx$$xxxx
